# VGPT2 Comprehensive Diagnostic Report

**Generated:** 2025-12-29
**Status:** Training Complete, Testing Complete, Improvement Plan Ready

---

## Executive Summary

VGPT2 is a LoRA fine-tuned Qwen2.5-7B-Instruct model trained on 23,742 records of Viewpoint Vista database documentation. The goal is to create an AI assistant that can accurately answer questions about the AP Wizard database schema and generate valid SQL queries.

### Current State

| Aspect | Status | Details |
|--------|--------|---------|
| **Training** | ‚úÖ Complete | 6h 35m, 7,050 steps, 5 epochs |
| **Model Accuracy** | ‚ö†Ô∏è 64% Good | 34/53 correct, 10/53 hallucinated |
| **Critical Issue** | üî¥ Hallucination | Model invents non-existent tables |
| **Deployment Ready** | ‚ùå No | Needs retraining with negative examples |

---

## 1. Project Overview

### 1.1 What is VGPT2?

VGPT2 (Virtual GPT 2) is designed to be an **AP Wizard Database Expert** - an AI that:
- Knows the Viewpoint Vista ERP database schema (1,000+ tables/views)
- Generates syntactically correct SQL Server queries
- Follows Viewpoint naming conventions (b-prefix, WITH NOLOCK, etc.)
- Understands business logic (batch processing, vendor groups, etc.)

### 1.2 Repository Location

```
C:\Github\LLM_fine-tuning\
‚îú‚îÄ‚îÄ automation/configs/vgpt2_lora_sft.yaml  # Training config
‚îú‚îÄ‚îÄ data/vgpt2_training_v2.json             # Training data (23,742 records)
‚îú‚îÄ‚îÄ saves/vgpt2_v2_lora_sft/                # Trained model (1.23 GB adapter)
‚îú‚îÄ‚îÄ scripts/test_vgpt2_comprehensive.py     # Test suite (53 questions)
‚îú‚îÄ‚îÄ output/vgpt2_test_results.json          # Test results
‚îî‚îÄ‚îÄ docs/VGPT2_Training_Data_Plan.md        # Data generation plan
```

---

## 2. Training Details

### 2.1 Model Architecture

| Component | Value |
|-----------|-------|
| **Base Model** | Qwen/Qwen2.5-7B-Instruct |
| **Parameters** | 7.9 billion |
| **Fine-tuning Method** | LoRA (Low-Rank Adaptation) |
| **LoRA Rank** | 128 |
| **LoRA Alpha** | 256 |
| **LoRA Target** | All linear layers |
| **Trainable Parameters** | 322,961,408 (4.07% of total) |
| **Adapter Size** | 1.23 GB |

### 2.2 Training Configuration

```yaml
# File: automation/configs/vgpt2_lora_sft.yaml
model_name_or_path: Qwen/Qwen2.5-7B-Instruct
finetuning_type: lora
lora_rank: 128
lora_alpha: 256
lora_target: all
dataset: vgpt2_v2
template: qwen
cutoff_len: 4096
per_device_train_batch_size: 4
gradient_accumulation_steps: 4  # Effective batch size = 16
learning_rate: 2.0e-4
num_train_epochs: 5.0
lr_scheduler_type: cosine
warmup_ratio: 0.1
bf16: true
val_size: 0.05
```

### 2.3 Training Results

| Metric | Value |
|--------|-------|
| **Total Steps** | 7,050 |
| **Epochs** | 5.0 |
| **Training Time** | 6h 35m (23,711 seconds) |
| **Final Train Loss** | 0.2454 |
| **Final Eval Loss** | 1.171 |
| **Samples/Second** | 4.756 |
| **Steps/Second** | 0.297 |

### 2.4 Hardware Used

| Component | Specification |
|-----------|---------------|
| **GPU** | NVIDIA RTX A6000 (48GB VRAM) |
| **CPU** | AMD Threadripper 7960X (48 cores) |
| **RAM** | 128 GB DDR5 |
| **OS** | Windows 11 |
| **VRAM Usage** | ~39.6 GB (82.6%) |
| **GPU Utilization** | 99% |

---

## 3. Training Data Analysis

### 3.1 Dataset Overview

| Metric | Value |
|--------|-------|
| **Total Records** | 23,742 |
| **Format** | Alpaca (instruction/input/output) |
| **Validation Split** | 5% (~1,187 samples) |
| **Source Repository** | C:\Github\VGPT2 |

### 3.2 Data Distribution by Category

| Category | Count | Percentage |
|----------|-------|------------|
| Schema Metadata | 7,898 | 33% |
| SP Documentation | 7,040 | 30% |
| View Documentation | 2,350 | 10% |
| SQL Generation | 2,018 | 8% |
| DDFI Forms | 1,692 | 7% |
| Crystal Report SQL | 1,217 | 5% |
| JOIN Patterns | 1,033 | 4% |
| Experts V2 | 405 | 2% |
| Canonical Rules | 26 | <1% |
| Naming Conventions | 26 | <1% |
| **TOTAL** | **23,742** | 100% |

### 3.3 Data Sources Used

| Source | Status | Records |
|--------|--------|---------|
| `_Metadata/columns.json` | ‚úÖ Used | ~8,000 |
| `Stored_Procedures/SP_Documentation/` | ‚úÖ Used | ~7,000 |
| `View/View_Documentation/` | ‚úÖ Used | ~2,350 |
| `_Relationship_Validation/` | ‚úÖ Used | ~1,000 |
| `Crystal_Reports_SQL/` | ‚úÖ Used | ~1,200 |
| `Experts/04_Experts/` | ‚ö†Ô∏è Partial | ~400 |

### 3.4 Data Sources NOT Used (Gaps)

| Source | Why Missing | Est. Records |
|--------|-------------|--------------|
| `Tables/` documentation | Not implemented | 1,600+ |
| `Functions/` documentation | Not implemented | 1,000+ |
| `Experts_V2/` full dataset | Only partial | 500+ |
| `_ai_orchestration/heuristics/` | Not implemented | 100+ |
| `Vista_KB/` articles | Not implemented | 2,000+ |
| **Negative examples** | **Never created** | **500+** |

---

## 4. Test Results Analysis

### 4.1 Test Methodology

- **Test Script:** `scripts/test_vgpt2_comprehensive.py`
- **Questions:** 53 total across 5 categories
- **Results File:** `output/vgpt2_test_results.json`
- **Generation Settings:** temperature=0.7, top_p=0.8, max_tokens=512

### 4.2 Results by Category

| Category | Total | ‚úÖ Good | ‚ö†Ô∏è Partial | ‚ùå Wrong |
|----------|-------|---------|------------|----------|
| Schema Knowledge | 15 | 10 (67%) | 3 (20%) | 2 (13%) |
| SQL Generation | 15 | 10 (67%) | 3 (20%) | 2 (13%) |
| Edge Cases (Hallucination) | 9 | 2 (22%) | 1 (11%) | **6 (67%)** |
| Naming Conventions | 6 | 5 (83%) | 1 (17%) | 0 (0%) |
| Business Logic | 8 | 7 (88%) | 1 (12%) | 0 (0%) |
| **TOTAL** | **53** | **34 (64%)** | **9 (17%)** | **10 (19%)** |

### 4.3 What's Working Well ‚úÖ

1. **Schema Knowledge** - Correctly identifies columns, data types, primary keys
2. **Naming Conventions** - Understands b-prefix, vrv*, module prefixes
3. **Business Logic** - Knows batch processing, VendorGroups, payment workflows
4. **Basic SQL** - Generates correct JOINs for known tables
5. **WITH (NOLOCK)** - Often includes correctly in queries

**Example Good Response:**
```
Q: "How do I join bAPTH and bAPVendor?"
A: "To join bAPTH and bAPVendor, use:
    FROM bAPTH
    INNER JOIN bAPVendor
      ON bAPTH.VendorGroup = bAPVendor.VendorGroup
      AND bAPTH.Vendor = bAPVendor.Vendor
    This pattern is used in the vrvAPRec_APEntry view."
```

### 4.4 Critical Issues Found üî¥

#### Issue #1: Hallucination on Non-Existent Tables (CRITICAL)

The model should say "does not exist" but instead **invents plausible-sounding answers**:

| Question | Expected Answer | Actual (WRONG) |
|----------|-----------------|----------------|
| "What is the Invoice table?" | "No table named 'Invoice' exists" | Made up detailed description about SM work orders |
| "How do I query UserPreferences table?" | "Table doesn't exist" | Gave full SQL with invented columns |
| "What columns are in Payments table?" | "No such table" | Listed 41 fake columns |
| "Describe CustomerOrders table" | "Doesn't exist" | Said "Customer Orders" (vague) |
| "SalesData table structure?" | "Doesn't exist" | Claimed it has Month, MonthID |
| "Join Invoice and Customer" | "These don't exist" | Gave JOIN referencing vrvSMWorkOrder |

**Root Cause:** Training data contains **zero negative examples** teaching the model to say "doesn't exist."

#### Issue #2: Terse/Incomplete Schema Responses

Some questions get unhelpfully brief answers:

| Question | Response | Problem |
|----------|----------|---------|
| "Describe the APTD table structure" | "AP Transaction Detail" | No columns listed |
| "Structure of PRTH table?" | "PR Timecard Header" | Just a name |
| "Describe GLDT table" | "GL Detail" | Useless |

**Root Cause:** Training data for these tables may lack detailed Q&A pairs.

#### Issue #3: SQL Syntax Errors

| Query | Error |
|-------|-------|
| Duplicate invoice check | Used `Invoice` column instead of `InvId` |
| AR outstanding balances | Returned Python code, not SQL |
| Equipment maintenance | Used Oracle-style `INTERVAL '1' YEAR` (invalid in SQL Server) |

---

## 5. Comprehensive Improvement Plan

### 5.1 Phase 1: Fix Hallucination (CRITICAL - Week 1)

**Goal:** Model should refuse to answer about non-existent tables

**Action:** Generate 500-1000 negative training examples

```json
{
  "instruction": "What columns are in the Invoice table?",
  "input": "",
  "output": "There is no table or view named 'Invoice' in Viewpoint Vista. You may be looking for:\n- bAPTH / APTH - AP Transaction Header (invoice headers)\n- vrvAP_MVAllInvoices - Report view for all AP invoices\n- APTL - AP Transaction Lines (invoice line items)\n\nPlease specify which object you need."
}
```

**Implementation Steps:**
1. Create list of 200+ common "fake" table names users might ask about
2. For each, generate Q&A with "doesn't exist" + suggestions
3. Add variations: "What is X?", "Describe X", "Columns in X", "Query X"
4. Target: 500-1000 negative examples

**Estimated Time:** 3-4 hours

### 5.2 Phase 2: Improve Schema Completeness (Week 1)

**Goal:** All major tables should have detailed responses

**Action:** Add detailed training examples for tables with poor responses

**Tables needing more examples:**
- APTD (AP Transaction Detail)
- PRTH (PR Transaction Header)
- GLDT (GL Detail)
- ARCM (AR Customer Master)
- EMEM (Equipment Master)

**Target:** 200-300 additional detailed schema examples

**Estimated Time:** 2-3 hours

### 5.3 Phase 3: SQL Syntax Validation (Week 1-2)

**Goal:** All generated SQL should be valid SQL Server syntax

**Actions:**
1. Review all SQL examples in training data
2. Replace Oracle/PostgreSQL syntax with SQL Server equivalents
3. Validate column names against `columns.json`
4. Add SQL Server-specific examples (DATEADD, DATEDIFF, etc.)

**Estimated Time:** 2-3 hours

### 5.4 Phase 4: Retrain Model (Week 2)

**Goal:** New model with improved accuracy

**Configuration Changes:**

| Parameter | Current | New | Rationale |
|-----------|---------|-----|-----------|
| Dataset size | 23,742 | ~25,500 | +500 negative, +300 schema |
| Epochs | 5 | 5 | Keep same |
| Learning rate | 2e-4 | 1.5e-4 | Slightly lower for stability |
| LoRA rank | 128 | 128 | Keep same |

**Estimated Time:** 6-7 hours training

### 5.5 Phase 5: Comprehensive Re-testing (Week 2)

**Goal:** Verify improvements

**Test Categories:**
1. Original 53 questions (should improve)
2. New hallucination tests (should pass)
3. Edge case SQL queries
4. Real-world user queries

**Success Criteria:**
- Overall accuracy: >80% (currently 64%)
- Hallucination rate: <5% (currently 67% on edge cases)
- No SQL syntax errors

### 5.6 Phase 6: Production Deployment (Week 3+)

**Option A: Fine-tuned Model Only**
- Simpler deployment
- Risk: May still hallucinate on rare queries

**Option B: Hybrid RAG + Fine-tuned Model (Recommended)**
- Fine-tuned model for business logic and conventions
- RAG retrieval for exact schema lookup
- Prevents hallucination by grounding in actual schema docs

---

## 6. File Inventory

### 6.1 Training Artifacts

| File | Location | Size | Purpose |
|------|----------|------|---------|
| Training config | `automation/configs/vgpt2_lora_sft.yaml` | 2 KB | Training settings |
| Training data | `data/vgpt2_training_v2.json` | 45 MB | 23,742 training records |
| Dataset registry | `data/dataset_info.json` | 5 KB | Dataset definitions |

### 6.2 Model Outputs

| File | Location | Size | Purpose |
|------|----------|------|---------|
| LoRA adapter | `saves/vgpt2_v2_lora_sft/adapter_model.safetensors` | 1.23 GB | Trained weights |
| Adapter config | `saves/vgpt2_v2_lora_sft/adapter_config.json` | <1 KB | LoRA config |
| Tokenizer | `saves/vgpt2_v2_lora_sft/tokenizer.json` | 10.9 MB | Tokenizer |
| Training loss plot | `saves/vgpt2_v2_lora_sft/training_loss.png` | - | Loss curve |
| Eval loss plot | `saves/vgpt2_v2_lora_sft/training_eval_loss.png` | - | Eval curve |
| Results | `saves/vgpt2_v2_lora_sft/all_results.json` | <1 KB | Final metrics |

### 6.3 Test Outputs

| File | Location | Purpose |
|------|----------|---------|
| Test script | `scripts/test_vgpt2_comprehensive.py` | 53-question test suite |
| Test results | `output/vgpt2_test_results.json` | All Q&A pairs with responses |

### 6.4 Documentation

| File | Purpose |
|------|---------|
| `VGPT2_PROJECT.md` | Main project documentation |
| `VGPT2_DIAGNOSTIC_REPORT.md` | This report |
| `SETUP.md` | Windows setup guide |
| `RESUME_TRAINING.md` | Checkpoint resume instructions |
| `docs/VGPT2_Training_Data_Plan.md` | Data generation strategy |

---

## 7. Quick Reference Commands

### Run Tests
```powershell
.\scripts\run_external.ps1 -Script "test_vgpt2_comprehensive.py"
```

### Interactive Chat
```powershell
.\scripts\run_external.ps1 -Command "llamafactory-cli chat --model_name_or_path Qwen/Qwen2.5-7B-Instruct --adapter_name_or_path saves/vgpt2_v2_lora_sft --template qwen --finetuning_type lora"
```

### Start Training
```powershell
.\scripts\start_training.ps1
```

### Resume Training
```powershell
.\scripts\start_training.ps1 -Resume
```

---

## 8. Recommended Next Steps

| Priority | Task | Effort | Impact |
|----------|------|--------|--------|
| üî¥ **1** | Generate negative examples (500+) | 3-4 hours | Critical - fixes hallucination |
| üü† **2** | Add detailed schema examples (300) | 2-3 hours | High - improves completeness |
| üü° **3** | Fix SQL syntax in training data | 2-3 hours | Medium - prevents errors |
| üü¢ **4** | Retrain model | 6-7 hours | High - applies fixes |
| üîµ **5** | Re-test with expanded suite | 1-2 hours | High - validates fixes |
| ‚ö™ **6** | Implement RAG hybrid (optional) | 1-2 days | Very High - production safety |

---

## 9. Success Metrics

| Metric | Current | Target | Status |
|--------|---------|--------|--------|
| Overall accuracy | 64% | >80% | ‚ùå |
| Hallucination rate | 67% (edge cases) | <5% | ‚ùå |
| Schema completeness | 67% | >90% | ‚ùå |
| SQL syntax errors | 13% | 0% | ‚ùå |
| Business logic accuracy | 88% | >95% | ‚ö†Ô∏è |
| Naming convention accuracy | 83% | >95% | ‚ö†Ô∏è |

---

*Report generated by comprehensive analysis of training artifacts, test results, and codebase review.*

